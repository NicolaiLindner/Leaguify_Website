{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:04:48.436378900Z",
     "start_time": "2023-09-16T13:04:48.433738300Z"
    }
   },
   "id": "f4241498fa3c643e"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "column_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "                \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "                \"hours-per-week\", \"native-country\", \"income\"]\n",
    "\n",
    "df = pd.read_csv(url, names=column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:04:53.129329300Z",
     "start_time": "2023-09-16T13:04:48.436378900Z"
    }
   },
   "id": "78d7cb2d1fdbcfc5"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "   age          workclass  fnlwgt   education  education-num  \\\n0   39          State-gov   77516   Bachelors             13   \n1   50   Self-emp-not-inc   83311   Bachelors             13   \n2   38            Private  215646     HS-grad              9   \n3   53            Private  234721        11th              7   \n4   28            Private  338409   Bachelors             13   \n\n        marital-status          occupation    relationship    race      sex  \\\n0        Never-married        Adm-clerical   Not-in-family   White     Male   \n1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n\n   capital-gain  capital-loss  hours-per-week  native-country  income  \n0          2174             0              40   United-States   <=50K  \n1             0             0              13   United-States   <=50K  \n2             0             0              40   United-States   <=50K  \n3             0             0              40   United-States   <=50K  \n4             0             0              40            Cuba   <=50K  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>State-gov</td>\n      <td>77516</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>Self-emp-not-inc</td>\n      <td>83311</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>215646</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Divorced</td>\n      <td>Handlers-cleaners</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>Private</td>\n      <td>234721</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Married-civ-spouse</td>\n      <td>Handlers-cleaners</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>Private</td>\n      <td>338409</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>Cuba</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first few rows of the dataset\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:04:53.143927300Z",
     "start_time": "2023-09-16T13:04:53.129329300Z"
    }
   },
   "id": "728626c2f9e7251c"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "((26048, 14), (6513, 14), (26048,), (6513,))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace '?' with NaN for easier handling of missing values\n",
    "df.replace('?', float('nan'), inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Label encode categorical features\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:04:53.265536600Z",
     "start_time": "2023-09-16T13:04:53.135927500Z"
    }
   },
   "id": "a04e311ded86a9e1"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.32600072026252747\n",
      "Epoch 2, Loss: 0.10940621793270111\n",
      "Epoch 3, Loss: 0.34294456243515015\n",
      "Epoch 4, Loss: 0.3308665156364441\n",
      "Epoch 5, Loss: 0.19017967581748962\n",
      "Epoch 6, Loss: 0.30140969157218933\n",
      "Epoch 7, Loss: 0.3186119794845581\n",
      "Epoch 8, Loss: 0.3690279424190521\n",
      "Epoch 9, Loss: 0.3106846213340759\n",
      "Epoch 10, Loss: 0.44470730423927307\n",
      "Epoch 11, Loss: 0.28237128257751465\n",
      "Epoch 12, Loss: 0.27928322553634644\n",
      "Epoch 13, Loss: 0.30924856662750244\n",
      "Epoch 14, Loss: 0.4148046374320984\n",
      "Epoch 15, Loss: 0.30627015233039856\n",
      "Epoch 16, Loss: 0.47651997208595276\n",
      "Epoch 17, Loss: 0.3488488495349884\n",
      "Epoch 18, Loss: 0.2099553793668747\n",
      "Epoch 19, Loss: 0.37201815843582153\n",
      "Epoch 20, Loss: 0.2185194492340088\n",
      "Epoch 21, Loss: 0.21945734322071075\n",
      "Epoch 22, Loss: 0.27121835947036743\n",
      "Epoch 23, Loss: 0.25044646859169006\n",
      "Epoch 24, Loss: 0.305257648229599\n",
      "Epoch 25, Loss: 0.14983657002449036\n",
      "Epoch 26, Loss: 0.3057785630226135\n",
      "Epoch 27, Loss: 0.26644259691238403\n",
      "Epoch 28, Loss: 0.2851184010505676\n",
      "Epoch 29, Loss: 0.2628222107887268\n",
      "Epoch 30, Loss: 0.19165866076946259\n",
      "Epoch 31, Loss: 0.24329404532909393\n",
      "Epoch 32, Loss: 0.37901580333709717\n",
      "Epoch 33, Loss: 0.3440192639827728\n",
      "Epoch 34, Loss: 0.2638557255268097\n",
      "Epoch 35, Loss: 0.1861274391412735\n",
      "Epoch 36, Loss: 0.308213472366333\n",
      "Epoch 37, Loss: 0.3599098026752472\n",
      "Epoch 38, Loss: 0.2524917423725128\n",
      "Epoch 39, Loss: 0.2888352870941162\n",
      "Epoch 40, Loss: 0.2498999834060669\n",
      "Epoch 41, Loss: 0.2889815866947174\n",
      "Epoch 42, Loss: 0.28540289402008057\n",
      "Epoch 43, Loss: 0.17781862616539001\n",
      "Epoch 44, Loss: 0.17862321436405182\n",
      "Epoch 45, Loss: 0.2887714207172394\n",
      "Epoch 46, Loss: 0.27088499069213867\n",
      "Epoch 47, Loss: 0.18052801489830017\n",
      "Epoch 48, Loss: 0.25897926092147827\n",
      "Epoch 49, Loss: 0.22652363777160645\n",
      "Epoch 50, Loss: 0.27497774362564087\n",
      "Epoch 51, Loss: 0.34719449281692505\n",
      "Epoch 52, Loss: 0.3421761393547058\n",
      "Epoch 53, Loss: 0.1848127394914627\n",
      "Epoch 54, Loss: 0.3261179029941559\n",
      "Epoch 55, Loss: 0.2731178104877472\n",
      "Epoch 56, Loss: 0.1858530193567276\n",
      "Epoch 57, Loss: 0.24389345943927765\n",
      "Epoch 58, Loss: 0.39740264415740967\n",
      "Epoch 59, Loss: 0.3219226002693176\n",
      "Epoch 60, Loss: 0.28986743092536926\n",
      "Epoch 61, Loss: 0.32883599400520325\n",
      "Epoch 62, Loss: 0.16143342852592468\n",
      "Epoch 63, Loss: 0.3111369013786316\n",
      "Epoch 64, Loss: 0.27681785821914673\n",
      "Epoch 65, Loss: 0.39024049043655396\n",
      "Epoch 66, Loss: 0.23076049983501434\n",
      "Epoch 67, Loss: 0.23720020055770874\n",
      "Epoch 68, Loss: 0.21755048632621765\n",
      "Epoch 69, Loss: 0.2311442494392395\n",
      "Epoch 70, Loss: 0.33984625339508057\n",
      "Epoch 71, Loss: 0.18876412510871887\n",
      "Epoch 72, Loss: 0.18666668236255646\n",
      "Epoch 73, Loss: 0.23319172859191895\n",
      "Epoch 74, Loss: 0.33550381660461426\n",
      "Epoch 75, Loss: 0.24490010738372803\n",
      "Epoch 76, Loss: 0.24693727493286133\n",
      "Epoch 77, Loss: 0.31893861293792725\n",
      "Epoch 78, Loss: 0.18935692310333252\n",
      "Epoch 79, Loss: 0.2327406406402588\n",
      "Epoch 80, Loss: 0.38635149598121643\n",
      "Epoch 81, Loss: 0.29942601919174194\n",
      "Epoch 82, Loss: 0.24609771370887756\n",
      "Epoch 83, Loss: 0.45240360498428345\n",
      "Epoch 84, Loss: 0.3618178963661194\n",
      "Epoch 85, Loss: 0.4459892809391022\n",
      "Epoch 86, Loss: 0.3016815483570099\n",
      "Epoch 87, Loss: 0.2416740208864212\n",
      "Epoch 88, Loss: 0.2861502170562744\n",
      "Epoch 89, Loss: 0.15913650393486023\n",
      "Epoch 90, Loss: 0.2109832763671875\n",
      "Epoch 91, Loss: 0.5183526873588562\n",
      "Epoch 92, Loss: 0.38083288073539734\n",
      "Epoch 93, Loss: 0.2428385466337204\n",
      "Epoch 94, Loss: 0.297590970993042\n",
      "Epoch 95, Loss: 0.27561455965042114\n",
      "Epoch 96, Loss: 0.23094019293785095\n",
      "Epoch 97, Loss: 0.31103813648223877\n",
      "Epoch 98, Loss: 0.3204639256000519\n",
      "Epoch 99, Loss: 0.13388291001319885\n",
      "Epoch 100, Loss: 0.22093075513839722\n",
      "Epoch 101, Loss: 0.19206953048706055\n",
      "Epoch 102, Loss: 0.20549902319908142\n",
      "Epoch 103, Loss: 0.3599998354911804\n",
      "Epoch 104, Loss: 0.17253121733665466\n",
      "Epoch 105, Loss: 0.2720450758934021\n",
      "Epoch 106, Loss: 0.26772642135620117\n",
      "Epoch 107, Loss: 0.28713348507881165\n",
      "Epoch 108, Loss: 0.17033585906028748\n",
      "Epoch 109, Loss: 0.16505584120750427\n",
      "Epoch 110, Loss: 0.3042348325252533\n",
      "Epoch 111, Loss: 0.4487435817718506\n",
      "Epoch 112, Loss: 0.22390222549438477\n",
      "Epoch 113, Loss: 0.20948904752731323\n",
      "Epoch 114, Loss: 0.17677219212055206\n",
      "Epoch 115, Loss: 0.3210397958755493\n",
      "Epoch 116, Loss: 0.3981207013130188\n",
      "Epoch 117, Loss: 0.20458835363388062\n",
      "Epoch 118, Loss: 0.25023701786994934\n",
      "Epoch 119, Loss: 0.17660346627235413\n",
      "Epoch 120, Loss: 0.3701748847961426\n",
      "Epoch 121, Loss: 0.14553944766521454\n",
      "Epoch 122, Loss: 0.10879726707935333\n",
      "Epoch 123, Loss: 0.165424183011055\n",
      "Epoch 124, Loss: 0.30298519134521484\n",
      "Epoch 125, Loss: 0.32792389392852783\n",
      "Epoch 126, Loss: 0.3274597227573395\n",
      "Epoch 127, Loss: 0.27752408385276794\n",
      "Epoch 128, Loss: 0.3474935293197632\n",
      "Epoch 129, Loss: 0.2138471156358719\n",
      "Epoch 130, Loss: 0.1976570039987564\n",
      "Epoch 131, Loss: 0.2707991302013397\n",
      "Epoch 132, Loss: 0.31322309374809265\n",
      "Epoch 133, Loss: 0.3302936851978302\n",
      "Epoch 134, Loss: 0.28546953201293945\n",
      "Epoch 135, Loss: 0.19894377887248993\n",
      "Epoch 136, Loss: 0.32915592193603516\n",
      "Epoch 137, Loss: 0.13268469274044037\n",
      "Epoch 138, Loss: 0.13757522404193878\n",
      "Epoch 139, Loss: 0.1636807769536972\n",
      "Epoch 140, Loss: 0.18596570193767548\n",
      "Epoch 141, Loss: 0.3239412009716034\n",
      "Epoch 142, Loss: 0.21928364038467407\n",
      "Epoch 143, Loss: 0.21136921644210815\n",
      "Epoch 144, Loss: 0.16583041846752167\n",
      "Epoch 145, Loss: 0.35212305188179016\n",
      "Epoch 146, Loss: 0.1929858922958374\n",
      "Epoch 147, Loss: 0.21112361550331116\n",
      "Epoch 148, Loss: 0.2258191704750061\n",
      "Epoch 149, Loss: 0.24169394373893738\n",
      "Epoch 150, Loss: 0.2445341795682907\n",
      "Epoch 151, Loss: 0.26376649737358093\n",
      "Epoch 152, Loss: 0.2387404441833496\n",
      "Epoch 153, Loss: 0.341297447681427\n",
      "Epoch 154, Loss: 0.15853138267993927\n",
      "Epoch 155, Loss: 0.17221501469612122\n",
      "Epoch 156, Loss: 0.3270164728164673\n",
      "Epoch 157, Loss: 0.218718022108078\n",
      "Epoch 158, Loss: 0.29623648524284363\n",
      "Epoch 159, Loss: 0.30757856369018555\n",
      "Epoch 160, Loss: 0.16736894845962524\n",
      "Epoch 161, Loss: 0.1670275181531906\n",
      "Epoch 162, Loss: 0.2718105614185333\n",
      "Epoch 163, Loss: 0.21375375986099243\n",
      "Epoch 164, Loss: 0.1445334106683731\n",
      "Epoch 165, Loss: 0.23494327068328857\n",
      "Epoch 166, Loss: 0.2524559497833252\n",
      "Epoch 167, Loss: 0.22016838192939758\n",
      "Epoch 168, Loss: 0.2674676477909088\n",
      "Epoch 169, Loss: 0.22052869200706482\n",
      "Epoch 170, Loss: 0.2376941442489624\n",
      "Epoch 171, Loss: 0.36126333475112915\n",
      "Epoch 172, Loss: 0.2781784236431122\n",
      "Epoch 173, Loss: 0.28143516182899475\n",
      "Epoch 174, Loss: 0.18886001408100128\n",
      "Epoch 175, Loss: 0.15489155054092407\n",
      "Epoch 176, Loss: 0.40065819025039673\n",
      "Epoch 177, Loss: 0.21127650141716003\n",
      "Epoch 178, Loss: 0.32514891028404236\n",
      "Epoch 179, Loss: 0.3652161657810211\n",
      "Epoch 180, Loss: 0.2891116142272949\n",
      "Epoch 181, Loss: 0.20905640721321106\n",
      "Epoch 182, Loss: 0.24433869123458862\n",
      "Epoch 183, Loss: 0.3419555723667145\n",
      "Epoch 184, Loss: 0.31410908699035645\n",
      "Epoch 185, Loss: 0.22270768880844116\n",
      "Epoch 186, Loss: 0.1560220569372177\n",
      "Epoch 187, Loss: 0.1881016343832016\n",
      "Epoch 188, Loss: 0.24501043558120728\n",
      "Epoch 189, Loss: 0.21888908743858337\n",
      "Epoch 190, Loss: 0.1773076355457306\n",
      "Epoch 191, Loss: 0.1844913363456726\n",
      "Epoch 192, Loss: 0.23638293147087097\n",
      "Epoch 193, Loss: 0.12582992017269135\n",
      "Epoch 194, Loss: 0.2115561068058014\n",
      "Epoch 195, Loss: 0.16196826100349426\n",
      "Epoch 196, Loss: 0.167530819773674\n",
      "Epoch 197, Loss: 0.21995587646961212\n",
      "Epoch 198, Loss: 0.39447781443595886\n",
      "Epoch 199, Loss: 0.3100076913833618\n",
      "Epoch 200, Loss: 0.26638928055763245\n",
      "Epoch 201, Loss: 0.15631990134716034\n",
      "Epoch 202, Loss: 0.23088328540325165\n",
      "Epoch 203, Loss: 0.11224246770143509\n",
      "Epoch 204, Loss: 0.26235878467559814\n",
      "Epoch 205, Loss: 0.19945257902145386\n",
      "Epoch 206, Loss: 0.16715341806411743\n",
      "Epoch 207, Loss: 0.19069808721542358\n",
      "Epoch 208, Loss: 0.1975044310092926\n",
      "Epoch 209, Loss: 0.18125158548355103\n",
      "Epoch 210, Loss: 0.2756642699241638\n",
      "Epoch 211, Loss: 0.32246536016464233\n",
      "Epoch 212, Loss: 0.2209886908531189\n",
      "Epoch 213, Loss: 0.1741144061088562\n",
      "Epoch 214, Loss: 0.12468726933002472\n",
      "Epoch 215, Loss: 0.2753284275531769\n",
      "Epoch 216, Loss: 0.30885785818099976\n",
      "Epoch 217, Loss: 0.35533782839775085\n",
      "Epoch 218, Loss: 0.1905888020992279\n",
      "Epoch 219, Loss: 0.21743838489055634\n",
      "Epoch 220, Loss: 0.2555791437625885\n",
      "Epoch 221, Loss: 0.19968892633914948\n",
      "Epoch 222, Loss: 0.16373538970947266\n",
      "Epoch 223, Loss: 0.26253992319107056\n",
      "Epoch 224, Loss: 0.24657918512821198\n",
      "Epoch 225, Loss: 0.24449843168258667\n",
      "Epoch 226, Loss: 0.31264108419418335\n",
      "Epoch 227, Loss: 0.313641220331192\n",
      "Epoch 228, Loss: 0.18477757275104523\n",
      "Epoch 229, Loss: 0.5147782564163208\n",
      "Epoch 230, Loss: 0.24090322852134705\n",
      "Epoch 231, Loss: 0.19621828198432922\n",
      "Epoch 232, Loss: 0.21963806450366974\n",
      "Epoch 233, Loss: 0.19074301421642303\n",
      "Epoch 234, Loss: 0.1719420701265335\n",
      "Epoch 235, Loss: 0.31107789278030396\n",
      "Epoch 236, Loss: 0.22857284545898438\n",
      "Epoch 237, Loss: 0.15935617685317993\n",
      "Epoch 238, Loss: 0.20079681277275085\n",
      "Epoch 239, Loss: 0.2049902081489563\n",
      "Epoch 240, Loss: 0.1450149565935135\n",
      "Epoch 241, Loss: 0.24647733569145203\n",
      "Epoch 242, Loss: 0.1456274688243866\n",
      "Epoch 243, Loss: 0.3564186692237854\n",
      "Epoch 244, Loss: 0.13823220133781433\n",
      "Epoch 245, Loss: 0.2063198983669281\n",
      "Epoch 246, Loss: 0.3052785396575928\n",
      "Epoch 247, Loss: 0.13664256036281586\n",
      "Epoch 248, Loss: 0.2198600322008133\n",
      "Epoch 249, Loss: 0.1273253858089447\n",
      "Epoch 250, Loss: 0.30211418867111206\n",
      "Epoch 251, Loss: 0.24245287477970123\n",
      "Epoch 252, Loss: 0.2639581859111786\n",
      "Epoch 253, Loss: 0.33142924308776855\n",
      "Epoch 254, Loss: 0.2360716313123703\n",
      "Epoch 255, Loss: 0.2145414501428604\n",
      "Epoch 256, Loss: 0.1537855863571167\n",
      "Epoch 257, Loss: 0.17657309770584106\n",
      "Epoch 258, Loss: 0.20910212397575378\n",
      "Epoch 259, Loss: 0.30384814739227295\n",
      "Epoch 260, Loss: 0.1691494584083557\n",
      "Epoch 261, Loss: 0.10876969993114471\n",
      "Epoch 262, Loss: 0.2624253034591675\n",
      "Epoch 263, Loss: 0.3360631465911865\n",
      "Epoch 264, Loss: 0.37546291947364807\n",
      "Epoch 265, Loss: 0.25461673736572266\n",
      "Epoch 266, Loss: 0.17722174525260925\n",
      "Epoch 267, Loss: 0.11062558740377426\n",
      "Epoch 268, Loss: 0.26348671317100525\n",
      "Epoch 269, Loss: 0.2913554608821869\n",
      "Epoch 270, Loss: 0.23637312650680542\n",
      "Epoch 271, Loss: 0.42380067706108093\n",
      "Epoch 272, Loss: 0.3018697500228882\n",
      "Epoch 273, Loss: 0.2011875957250595\n",
      "Epoch 274, Loss: 0.355392187833786\n",
      "Epoch 275, Loss: 0.1698564887046814\n",
      "Epoch 276, Loss: 0.26524701714515686\n",
      "Epoch 277, Loss: 0.28117695450782776\n",
      "Epoch 278, Loss: 0.23896706104278564\n",
      "Epoch 279, Loss: 0.13933013379573822\n",
      "Epoch 280, Loss: 0.18365810811519623\n",
      "Epoch 281, Loss: 0.13265763223171234\n",
      "Epoch 282, Loss: 0.1570974588394165\n",
      "Epoch 283, Loss: 0.1126454770565033\n",
      "Epoch 284, Loss: 0.2908787131309509\n",
      "Epoch 285, Loss: 0.20155367255210876\n",
      "Epoch 286, Loss: 0.29738324880599976\n",
      "Epoch 287, Loss: 0.319757878780365\n",
      "Epoch 288, Loss: 0.2155851423740387\n",
      "Epoch 289, Loss: 0.16949833929538727\n",
      "Epoch 290, Loss: 0.1870480626821518\n",
      "Epoch 291, Loss: 0.42964786291122437\n",
      "Epoch 292, Loss: 0.13294155895709991\n",
      "Epoch 293, Loss: 0.09104391187429428\n",
      "Epoch 294, Loss: 0.11954055726528168\n",
      "Epoch 295, Loss: 0.1206452026963234\n",
      "Epoch 296, Loss: 0.15957914292812347\n",
      "Epoch 297, Loss: 0.12381268292665482\n",
      "Epoch 298, Loss: 0.3044288754463196\n",
      "Epoch 299, Loss: 0.27020740509033203\n",
      "Epoch 300, Loss: 0.18895001709461212\n",
      "Epoch 301, Loss: 0.21230123937129974\n",
      "Epoch 302, Loss: 0.24858519434928894\n",
      "Epoch 303, Loss: 0.35646897554397583\n",
      "Epoch 304, Loss: 0.1724221557378769\n",
      "Epoch 305, Loss: 0.3594758212566376\n",
      "Epoch 306, Loss: 0.1606576293706894\n",
      "Epoch 307, Loss: 0.20944321155548096\n",
      "Epoch 308, Loss: 0.3844785690307617\n",
      "Epoch 309, Loss: 0.1625250279903412\n",
      "Epoch 310, Loss: 0.23757506906986237\n",
      "Epoch 311, Loss: 0.15095898509025574\n",
      "Epoch 312, Loss: 0.2734553813934326\n",
      "Epoch 313, Loss: 0.21923485398292542\n",
      "Epoch 314, Loss: 0.06783918291330338\n",
      "Epoch 315, Loss: 0.361683189868927\n",
      "Epoch 316, Loss: 0.28256699442863464\n",
      "Epoch 317, Loss: 0.13176852464675903\n",
      "Epoch 318, Loss: 0.1459410935640335\n",
      "Epoch 319, Loss: 0.33668991923332214\n",
      "Epoch 320, Loss: 0.24937556684017181\n",
      "Epoch 321, Loss: 0.21324336528778076\n",
      "Epoch 322, Loss: 0.13217715919017792\n",
      "Epoch 323, Loss: 0.2825523614883423\n",
      "Epoch 324, Loss: 0.24897131323814392\n",
      "Epoch 325, Loss: 0.29523441195487976\n",
      "Epoch 326, Loss: 0.10947497934103012\n",
      "Epoch 327, Loss: 0.14982113242149353\n",
      "Epoch 328, Loss: 0.11884072422981262\n",
      "Epoch 329, Loss: 0.17131873965263367\n",
      "Epoch 330, Loss: 0.12520341575145721\n",
      "Epoch 331, Loss: 0.16499128937721252\n",
      "Epoch 332, Loss: 0.23164308071136475\n",
      "Epoch 333, Loss: 0.23707598447799683\n",
      "Epoch 334, Loss: 0.22570466995239258\n",
      "Epoch 335, Loss: 0.06953468173742294\n",
      "Epoch 336, Loss: 0.12038663774728775\n",
      "Epoch 337, Loss: 0.2866097390651703\n",
      "Epoch 338, Loss: 0.14545594155788422\n",
      "Epoch 339, Loss: 0.19864217936992645\n",
      "Epoch 340, Loss: 0.15663059055805206\n",
      "Epoch 341, Loss: 0.12668339908123016\n",
      "Epoch 342, Loss: 0.22480560839176178\n",
      "Epoch 343, Loss: 0.2373029887676239\n",
      "Epoch 344, Loss: 0.3187590539455414\n",
      "Epoch 345, Loss: 0.3660908639431\n",
      "Epoch 346, Loss: 0.0571749210357666\n",
      "Epoch 347, Loss: 0.17908459901809692\n",
      "Epoch 348, Loss: 0.22183728218078613\n",
      "Epoch 349, Loss: 0.23595277965068817\n",
      "Epoch 350, Loss: 0.1616329401731491\n",
      "Epoch 351, Loss: 0.23067280650138855\n",
      "Epoch 352, Loss: 0.051362477242946625\n",
      "Epoch 353, Loss: 0.16977959871292114\n",
      "Epoch 354, Loss: 0.11685852706432343\n",
      "Epoch 355, Loss: 0.21929872035980225\n",
      "Epoch 356, Loss: 0.22255530953407288\n",
      "Epoch 357, Loss: 0.34134313464164734\n",
      "Epoch 358, Loss: 0.14375728368759155\n",
      "Epoch 359, Loss: 0.12726011872291565\n",
      "Epoch 360, Loss: 0.38383036851882935\n",
      "Epoch 361, Loss: 0.12012118101119995\n",
      "Epoch 362, Loss: 0.22039318084716797\n",
      "Epoch 363, Loss: 0.22603411972522736\n",
      "Epoch 364, Loss: 0.23288239538669586\n",
      "Epoch 365, Loss: 0.19923388957977295\n",
      "Epoch 366, Loss: 0.300851047039032\n",
      "Epoch 367, Loss: 0.14572134613990784\n",
      "Epoch 368, Loss: 0.16198965907096863\n",
      "Epoch 369, Loss: 0.1647852212190628\n",
      "Epoch 370, Loss: 0.21303237974643707\n",
      "Epoch 371, Loss: 0.18087579309940338\n",
      "Epoch 372, Loss: 0.2993527054786682\n",
      "Epoch 373, Loss: 0.3257808983325958\n",
      "Epoch 374, Loss: 0.09387745708227158\n",
      "Epoch 375, Loss: 0.11155581474304199\n",
      "Epoch 376, Loss: 0.09990806877613068\n",
      "Epoch 377, Loss: 0.3081195056438446\n",
      "Epoch 378, Loss: 0.17374776303768158\n",
      "Epoch 379, Loss: 0.2625221610069275\n",
      "Epoch 380, Loss: 0.15189996361732483\n",
      "Epoch 381, Loss: 0.3524830937385559\n",
      "Epoch 382, Loss: 0.5101668238639832\n",
      "Epoch 383, Loss: 0.49379962682724\n",
      "Epoch 384, Loss: 0.27409785985946655\n",
      "Epoch 385, Loss: 0.2521781921386719\n",
      "Epoch 386, Loss: 0.21532884240150452\n",
      "Epoch 387, Loss: 0.29988566040992737\n",
      "Epoch 388, Loss: 0.15337102115154266\n",
      "Epoch 389, Loss: 0.20151452720165253\n",
      "Epoch 390, Loss: 0.1335582137107849\n",
      "Epoch 391, Loss: 0.17620037496089935\n",
      "Epoch 392, Loss: 0.2799268662929535\n",
      "Epoch 393, Loss: 0.2679732143878937\n",
      "Epoch 394, Loss: 0.11497075110673904\n",
      "Epoch 395, Loss: 0.13647215068340302\n",
      "Epoch 396, Loss: 0.2331695556640625\n",
      "Epoch 397, Loss: 0.16622023284435272\n",
      "Epoch 398, Loss: 0.11896329373121262\n",
      "Epoch 399, Loss: 0.2204269915819168\n",
      "Epoch 400, Loss: 0.208144873380661\n",
      "Epoch 401, Loss: 0.2238425612449646\n",
      "Epoch 402, Loss: 0.16016313433647156\n",
      "Epoch 403, Loss: 0.21266762912273407\n",
      "Epoch 404, Loss: 0.2297438383102417\n",
      "Epoch 405, Loss: 0.23416811227798462\n",
      "Epoch 406, Loss: 0.24411621689796448\n",
      "Epoch 407, Loss: 0.19451561570167542\n",
      "Epoch 408, Loss: 0.18956150114536285\n",
      "Epoch 409, Loss: 0.2827606797218323\n",
      "Epoch 410, Loss: 0.12316834926605225\n",
      "Epoch 411, Loss: 0.14806382358074188\n",
      "Epoch 412, Loss: 0.08741044253110886\n",
      "Epoch 413, Loss: 0.2096899151802063\n",
      "Epoch 414, Loss: 0.10647924244403839\n",
      "Epoch 415, Loss: 0.15404996275901794\n",
      "Epoch 416, Loss: 0.3137509226799011\n",
      "Epoch 417, Loss: 0.21704569458961487\n",
      "Epoch 418, Loss: 0.1827196478843689\n",
      "Epoch 419, Loss: 0.0901729017496109\n",
      "Epoch 420, Loss: 0.07822048664093018\n",
      "Epoch 421, Loss: 0.1626124083995819\n",
      "Epoch 422, Loss: 0.2127482146024704\n",
      "Epoch 423, Loss: 0.18632692098617554\n",
      "Epoch 424, Loss: 0.14427831768989563\n",
      "Epoch 425, Loss: 0.22801461815834045\n",
      "Epoch 426, Loss: 0.16612359881401062\n",
      "Epoch 427, Loss: 0.09263990074396133\n",
      "Epoch 428, Loss: 0.22906921803951263\n",
      "Epoch 429, Loss: 0.1782771348953247\n",
      "Epoch 430, Loss: 0.3545536696910858\n",
      "Epoch 431, Loss: 0.2742227017879486\n",
      "Epoch 432, Loss: 0.19195501506328583\n",
      "Epoch 433, Loss: 0.3096200227737427\n",
      "Epoch 434, Loss: 0.16738049685955048\n",
      "Epoch 435, Loss: 0.11587643623352051\n",
      "Epoch 436, Loss: 0.3765058219432831\n",
      "Epoch 437, Loss: 0.16235315799713135\n",
      "Epoch 438, Loss: 0.1630922108888626\n",
      "Epoch 439, Loss: 0.191048264503479\n",
      "Epoch 440, Loss: 0.19796256721019745\n",
      "Epoch 441, Loss: 0.2820606827735901\n",
      "Epoch 442, Loss: 0.14217887818813324\n",
      "Epoch 443, Loss: 0.30589911341667175\n",
      "Epoch 444, Loss: 0.14896249771118164\n",
      "Epoch 445, Loss: 0.3691451847553253\n",
      "Epoch 446, Loss: 0.1826133131980896\n",
      "Epoch 447, Loss: 0.2827165722846985\n",
      "Epoch 448, Loss: 0.23016232252120972\n",
      "Epoch 449, Loss: 0.2763630449771881\n",
      "Epoch 450, Loss: 0.3167732357978821\n",
      "Epoch 451, Loss: 0.1757197231054306\n",
      "Epoch 452, Loss: 0.33027151226997375\n",
      "Epoch 453, Loss: 0.16938821971416473\n",
      "Epoch 454, Loss: 0.24922728538513184\n",
      "Epoch 455, Loss: 0.1951710432767868\n",
      "Epoch 456, Loss: 0.14114037156105042\n",
      "Epoch 457, Loss: 0.24898478388786316\n",
      "Epoch 458, Loss: 0.3928898572921753\n",
      "Epoch 459, Loss: 0.20223334431648254\n",
      "Epoch 460, Loss: 0.23505799472332\n",
      "Epoch 461, Loss: 0.25141704082489014\n",
      "Epoch 462, Loss: 0.2899317741394043\n",
      "Epoch 463, Loss: 0.1703016757965088\n",
      "Epoch 464, Loss: 0.14368127286434174\n",
      "Epoch 465, Loss: 0.13830406963825226\n",
      "Epoch 466, Loss: 0.19384104013442993\n",
      "Epoch 467, Loss: 0.092679962515831\n",
      "Epoch 468, Loss: 0.23779794573783875\n",
      "Epoch 469, Loss: 0.15073175728321075\n",
      "Epoch 470, Loss: 0.2994207739830017\n",
      "Epoch 471, Loss: 0.24614782631397247\n",
      "Epoch 472, Loss: 0.24491336941719055\n",
      "Epoch 473, Loss: 0.2295062243938446\n",
      "Epoch 474, Loss: 0.1893680840730667\n",
      "Epoch 475, Loss: 0.4047375023365021\n",
      "Epoch 476, Loss: 0.19752676784992218\n",
      "Epoch 477, Loss: 0.0896344855427742\n",
      "Epoch 478, Loss: 0.24449191987514496\n",
      "Epoch 479, Loss: 0.04147612303495407\n",
      "Epoch 480, Loss: 0.13999050855636597\n",
      "Epoch 481, Loss: 0.23773276805877686\n",
      "Epoch 482, Loss: 0.16353058815002441\n",
      "Epoch 483, Loss: 0.18151958286762238\n",
      "Epoch 484, Loss: 0.11687827110290527\n",
      "Epoch 485, Loss: 0.1700255423784256\n",
      "Epoch 486, Loss: 0.04713325947523117\n",
      "Epoch 487, Loss: 0.14360639452934265\n",
      "Epoch 488, Loss: 0.22109222412109375\n",
      "Epoch 489, Loss: 0.17886902391910553\n",
      "Epoch 490, Loss: 0.08548711240291595\n",
      "Epoch 491, Loss: 0.14868153631687164\n",
      "Epoch 492, Loss: 0.25374704599380493\n",
      "Epoch 493, Loss: 0.2541503310203552\n",
      "Epoch 494, Loss: 0.12075534462928772\n",
      "Epoch 495, Loss: 0.14039914309978485\n",
      "Epoch 496, Loss: 0.21456439793109894\n",
      "Epoch 497, Loss: 0.157759889960289\n",
      "Epoch 498, Loss: 0.2558434009552002\n",
      "Epoch 499, Loss: 0.307147741317749\n",
      "Epoch 500, Loss: 0.14176693558692932\n",
      "Epoch 501, Loss: 0.15996795892715454\n",
      "Epoch 502, Loss: 0.21415254473686218\n",
      "Epoch 503, Loss: 0.1795545518398285\n",
      "Epoch 504, Loss: 0.2950979173183441\n",
      "Epoch 505, Loss: 0.21212488412857056\n",
      "Epoch 506, Loss: 0.3384271264076233\n",
      "Epoch 507, Loss: 0.20033127069473267\n",
      "Epoch 508, Loss: 0.2836854159832001\n",
      "Epoch 509, Loss: 0.16912299394607544\n",
      "Epoch 510, Loss: 0.22700388729572296\n",
      "Epoch 511, Loss: 0.1880188137292862\n",
      "Epoch 512, Loss: 0.2815030515193939\n",
      "Epoch 513, Loss: 0.06604313850402832\n",
      "Epoch 514, Loss: 0.14337633550167084\n",
      "Epoch 515, Loss: 0.10224445164203644\n",
      "Epoch 516, Loss: 0.15675699710845947\n",
      "Epoch 517, Loss: 0.25425195693969727\n",
      "Epoch 518, Loss: 0.12189186364412308\n",
      "Epoch 519, Loss: 0.13846592605113983\n",
      "Epoch 520, Loss: 0.2609842121601105\n",
      "Epoch 521, Loss: 0.27929773926734924\n",
      "Epoch 522, Loss: 0.24348849058151245\n",
      "Epoch 523, Loss: 0.20336908102035522\n",
      "Epoch 524, Loss: 0.22553126513957977\n",
      "Epoch 525, Loss: 0.18740667402744293\n",
      "Epoch 526, Loss: 0.14323678612709045\n",
      "Epoch 527, Loss: 0.2768005430698395\n",
      "Epoch 528, Loss: 0.2615121603012085\n",
      "Epoch 529, Loss: 0.17314030230045319\n",
      "Epoch 530, Loss: 0.214959979057312\n",
      "Epoch 531, Loss: 0.2283637821674347\n",
      "Epoch 532, Loss: 0.20700137317180634\n",
      "Epoch 533, Loss: 0.12343571335077286\n",
      "Epoch 534, Loss: 0.15505284070968628\n",
      "Epoch 535, Loss: 0.16169607639312744\n",
      "Epoch 536, Loss: 0.14338313043117523\n",
      "Epoch 537, Loss: 0.22741328179836273\n",
      "Epoch 538, Loss: 0.3643438220024109\n",
      "Epoch 539, Loss: 0.12358301877975464\n",
      "Epoch 540, Loss: 0.22363591194152832\n",
      "Epoch 541, Loss: 0.05364018678665161\n",
      "Epoch 542, Loss: 0.36692848801612854\n",
      "Epoch 543, Loss: 0.2313893437385559\n",
      "Epoch 544, Loss: 0.29421523213386536\n",
      "Epoch 545, Loss: 0.1840202957391739\n",
      "Epoch 546, Loss: 0.08215592801570892\n",
      "Epoch 547, Loss: 0.257663756608963\n",
      "Epoch 548, Loss: 0.22082014381885529\n",
      "Epoch 549, Loss: 0.24836967885494232\n",
      "Epoch 550, Loss: 0.21421170234680176\n",
      "Epoch 551, Loss: 0.10863155126571655\n",
      "Epoch 552, Loss: 0.2409336417913437\n",
      "Epoch 553, Loss: 0.2310498058795929\n",
      "Epoch 554, Loss: 0.2629753053188324\n",
      "Epoch 555, Loss: 0.058345209807157516\n",
      "Epoch 556, Loss: 0.30076485872268677\n",
      "Epoch 557, Loss: 0.176219642162323\n",
      "Epoch 558, Loss: 0.237152561545372\n",
      "Epoch 559, Loss: 0.15301178395748138\n",
      "Epoch 560, Loss: 0.16118468344211578\n",
      "Epoch 561, Loss: 0.09174658358097076\n",
      "Epoch 562, Loss: 0.22012105584144592\n",
      "Epoch 563, Loss: 0.15853966772556305\n",
      "Epoch 564, Loss: 0.18159888684749603\n",
      "Epoch 565, Loss: 0.15436062216758728\n",
      "Epoch 566, Loss: 0.2701340913772583\n",
      "Epoch 567, Loss: 0.10405750572681427\n",
      "Epoch 568, Loss: 0.32863283157348633\n",
      "Epoch 569, Loss: 0.2451706975698471\n",
      "Epoch 570, Loss: 0.14321540296077728\n",
      "Epoch 571, Loss: 0.1995856910943985\n",
      "Epoch 572, Loss: 0.2289428561925888\n",
      "Epoch 573, Loss: 0.13390123844146729\n",
      "Epoch 574, Loss: 0.21023619174957275\n",
      "Epoch 575, Loss: 0.12925778329372406\n",
      "Epoch 576, Loss: 0.22941814363002777\n",
      "Epoch 577, Loss: 0.22671936452388763\n",
      "Epoch 578, Loss: 0.2676262855529785\n",
      "Epoch 579, Loss: 0.2577112913131714\n",
      "Epoch 580, Loss: 0.08270684629678726\n",
      "Epoch 581, Loss: 0.24988794326782227\n",
      "Epoch 582, Loss: 0.2637521028518677\n",
      "Epoch 583, Loss: 0.15716741979122162\n",
      "Epoch 584, Loss: 0.13747037947177887\n",
      "Epoch 585, Loss: 0.22455108165740967\n",
      "Epoch 586, Loss: 0.21034912765026093\n",
      "Epoch 587, Loss: 0.2655993700027466\n",
      "Epoch 588, Loss: 0.1875997632741928\n",
      "Epoch 589, Loss: 0.2734062969684601\n",
      "Epoch 590, Loss: 0.19304855167865753\n",
      "Epoch 591, Loss: 0.19747644662857056\n",
      "Epoch 592, Loss: 0.12844085693359375\n",
      "Epoch 593, Loss: 0.4476463496685028\n",
      "Epoch 594, Loss: 0.21879678964614868\n",
      "Epoch 595, Loss: 0.13519355654716492\n",
      "Epoch 596, Loss: 0.3081967234611511\n",
      "Epoch 597, Loss: 0.12327559292316437\n",
      "Epoch 598, Loss: 0.30548107624053955\n",
      "Epoch 599, Loss: 0.10019467771053314\n",
      "Epoch 600, Loss: 0.2429570108652115\n",
      "Epoch 601, Loss: 0.20653322339057922\n",
      "Epoch 602, Loss: 0.14342068135738373\n",
      "Epoch 603, Loss: 0.1999775767326355\n",
      "Epoch 604, Loss: 0.2663072645664215\n",
      "Epoch 605, Loss: 0.3297954201698303\n",
      "Epoch 606, Loss: 0.3928993344306946\n",
      "Epoch 607, Loss: 0.18320927023887634\n",
      "Epoch 608, Loss: 0.4117814898490906\n",
      "Epoch 609, Loss: 0.15619511902332306\n",
      "Epoch 610, Loss: 0.1580532342195511\n",
      "Epoch 611, Loss: 0.23633792996406555\n",
      "Epoch 612, Loss: 0.14660879969596863\n",
      "Epoch 613, Loss: 0.13479167222976685\n",
      "Epoch 614, Loss: 0.13839435577392578\n",
      "Epoch 615, Loss: 0.1943325698375702\n",
      "Epoch 616, Loss: 0.17261961102485657\n",
      "Epoch 617, Loss: 0.11362951248884201\n",
      "Epoch 618, Loss: 0.09002318978309631\n",
      "Epoch 619, Loss: 0.16670626401901245\n",
      "Epoch 620, Loss: 0.10447388142347336\n",
      "Epoch 621, Loss: 0.18693837523460388\n",
      "Epoch 622, Loss: 0.18003112077713013\n",
      "Epoch 623, Loss: 0.07923316210508347\n",
      "Epoch 624, Loss: 0.38011425733566284\n",
      "Epoch 625, Loss: 0.15705302357673645\n",
      "Epoch 626, Loss: 0.12926313281059265\n",
      "Epoch 627, Loss: 0.2006113976240158\n",
      "Epoch 628, Loss: 0.29737332463264465\n",
      "Epoch 629, Loss: 0.2051474153995514\n",
      "Epoch 630, Loss: 0.18187814950942993\n",
      "Epoch 631, Loss: 0.13063743710517883\n",
      "Epoch 632, Loss: 0.11061078310012817\n",
      "Epoch 633, Loss: 0.08995847404003143\n",
      "Epoch 634, Loss: 0.1637331247329712\n",
      "Epoch 635, Loss: 0.09953363984823227\n",
      "Epoch 636, Loss: 0.2602337896823883\n",
      "Epoch 637, Loss: 0.13513948023319244\n",
      "Epoch 638, Loss: 0.142950177192688\n",
      "Epoch 639, Loss: 0.15028157830238342\n",
      "Epoch 640, Loss: 0.09468705952167511\n",
      "Epoch 641, Loss: 0.08405769616365433\n",
      "Epoch 642, Loss: 0.1888861060142517\n",
      "Epoch 643, Loss: 0.08248816430568695\n",
      "Epoch 644, Loss: 0.14721019566059113\n",
      "Epoch 645, Loss: 0.22813956439495087\n",
      "Epoch 646, Loss: 0.21424750983715057\n",
      "Epoch 647, Loss: 0.10697720944881439\n",
      "Epoch 648, Loss: 0.11053137481212616\n",
      "Epoch 649, Loss: 0.2194286733865738\n",
      "Epoch 650, Loss: 0.06298242509365082\n",
      "Epoch 651, Loss: 0.18635520339012146\n",
      "Epoch 652, Loss: 0.29854366183280945\n",
      "Epoch 653, Loss: 0.15268966555595398\n",
      "Epoch 654, Loss: 0.22355826199054718\n",
      "Epoch 655, Loss: 0.19592177867889404\n",
      "Epoch 656, Loss: 0.2775082290172577\n",
      "Epoch 657, Loss: 0.16634100675582886\n",
      "Epoch 658, Loss: 0.13697722554206848\n",
      "Epoch 659, Loss: 0.528251051902771\n",
      "Epoch 660, Loss: 0.13989542424678802\n",
      "Epoch 661, Loss: 0.14909687638282776\n",
      "Epoch 662, Loss: 0.14894992113113403\n",
      "Epoch 663, Loss: 0.1514093428850174\n",
      "Epoch 664, Loss: 0.264495313167572\n",
      "Epoch 665, Loss: 0.23418518900871277\n",
      "Epoch 666, Loss: 0.10312031954526901\n",
      "Epoch 667, Loss: 0.28981998562812805\n",
      "Epoch 668, Loss: 0.08790606260299683\n",
      "Epoch 669, Loss: 0.2567266523838043\n",
      "Epoch 670, Loss: 0.16735553741455078\n",
      "Epoch 671, Loss: 0.13815340399742126\n",
      "Epoch 672, Loss: 0.0889548659324646\n",
      "Epoch 673, Loss: 0.24871280789375305\n",
      "Epoch 674, Loss: 0.24021179974079132\n",
      "Epoch 675, Loss: 0.16326378285884857\n",
      "Epoch 676, Loss: 0.13640020787715912\n",
      "Epoch 677, Loss: 0.1542983502149582\n",
      "Epoch 678, Loss: 0.12780015170574188\n",
      "Epoch 679, Loss: 0.2130393534898758\n",
      "Epoch 680, Loss: 0.11162583529949188\n",
      "Epoch 681, Loss: 0.2631217837333679\n",
      "Epoch 682, Loss: 0.2084926962852478\n",
      "Epoch 683, Loss: 0.18983235955238342\n",
      "Epoch 684, Loss: 0.172690287232399\n",
      "Epoch 685, Loss: 0.21912114322185516\n",
      "Epoch 686, Loss: 0.23748686909675598\n",
      "Epoch 687, Loss: 0.28564974665641785\n",
      "Epoch 688, Loss: 0.15892229974269867\n",
      "Epoch 689, Loss: 0.20889787375926971\n",
      "Epoch 690, Loss: 0.19578838348388672\n",
      "Epoch 691, Loss: 0.1571514904499054\n",
      "Epoch 692, Loss: 0.21839797496795654\n",
      "Epoch 693, Loss: 0.3176402151584625\n",
      "Epoch 694, Loss: 0.25428083539009094\n",
      "Epoch 695, Loss: 0.17352300882339478\n",
      "Epoch 696, Loss: 0.19361786544322968\n",
      "Epoch 697, Loss: 0.2097889930009842\n",
      "Epoch 698, Loss: 0.27667275071144104\n",
      "Epoch 699, Loss: 0.19928452372550964\n",
      "Epoch 700, Loss: 0.17297334969043732\n",
      "Epoch 701, Loss: 0.19099511206150055\n",
      "Epoch 702, Loss: 0.241448312997818\n",
      "Epoch 703, Loss: 0.22896398603916168\n",
      "Epoch 704, Loss: 0.1518825888633728\n",
      "Epoch 705, Loss: 0.11292761564254761\n",
      "Epoch 706, Loss: 0.20351552963256836\n",
      "Epoch 707, Loss: 0.06456223130226135\n",
      "Epoch 708, Loss: 0.19832393527030945\n",
      "Epoch 709, Loss: 0.26509758830070496\n",
      "Epoch 710, Loss: 0.21127532422542572\n",
      "Epoch 711, Loss: 0.11635667830705643\n",
      "Epoch 712, Loss: 0.21262086927890778\n",
      "Epoch 713, Loss: 0.15305328369140625\n",
      "Epoch 714, Loss: 0.1167575791478157\n",
      "Epoch 715, Loss: 0.25463220477104187\n",
      "Epoch 716, Loss: 0.21912561357021332\n",
      "Epoch 717, Loss: 0.21112222969532013\n",
      "Epoch 718, Loss: 0.19318918883800507\n",
      "Epoch 719, Loss: 0.23165658116340637\n",
      "Epoch 720, Loss: 0.170871764421463\n",
      "Epoch 721, Loss: 0.12496890127658844\n",
      "Epoch 722, Loss: 0.1363409012556076\n",
      "Epoch 723, Loss: 0.1592744141817093\n",
      "Epoch 724, Loss: 0.35001903772354126\n",
      "Epoch 725, Loss: 0.1873578131198883\n",
      "Epoch 726, Loss: 0.07399947196245193\n",
      "Epoch 727, Loss: 0.16480962932109833\n",
      "Epoch 728, Loss: 0.06518752127885818\n",
      "Epoch 729, Loss: 0.12761269509792328\n",
      "Epoch 730, Loss: 0.3112637400627136\n",
      "Epoch 731, Loss: 0.20028288662433624\n",
      "Epoch 732, Loss: 0.09507444500923157\n",
      "Epoch 733, Loss: 0.35434040427207947\n",
      "Epoch 734, Loss: 0.15497423708438873\n",
      "Epoch 735, Loss: 0.2770916223526001\n",
      "Epoch 736, Loss: 0.3397582769393921\n",
      "Epoch 737, Loss: 0.11184603720903397\n",
      "Epoch 738, Loss: 0.12279865890741348\n",
      "Epoch 739, Loss: 0.2254839837551117\n",
      "Epoch 740, Loss: 0.14998354017734528\n",
      "Epoch 741, Loss: 0.133957177400589\n",
      "Epoch 742, Loss: 0.2486802637577057\n",
      "Epoch 743, Loss: 0.1320781409740448\n",
      "Epoch 744, Loss: 0.20469599962234497\n",
      "Epoch 745, Loss: 0.1421213150024414\n",
      "Epoch 746, Loss: 0.16468220949172974\n",
      "Epoch 747, Loss: 0.1866765022277832\n",
      "Epoch 748, Loss: 0.1003713458776474\n",
      "Epoch 749, Loss: 0.1820177435874939\n",
      "Epoch 750, Loss: 0.39958319067955017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 37\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5000\u001B[39m):\n\u001B[1;32m---> 37\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (data, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m     38\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     39\u001B[0m         output \u001B[38;5;241m=\u001B[39m model(data)\n",
      "File \u001B[1;32mC:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mC:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    205\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msamples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtransposed\u001B[49m\u001B[43m]\u001B[49m  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 119\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    122\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32mC:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    160\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    161\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 162\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchbnn as bnn\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_data = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the Bayesian Neural Network\n",
    "class BayesianNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BayesianNN, self).__init__()\n",
    "        self.layer1 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=14, out_features=64)\n",
    "        self.layer2 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=64, out_features=32)\n",
    "        self.layer3 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=32, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = BayesianNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5000):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        # Option 1: Reshape output\n",
    "        output = output.squeeze()\n",
    "        \n",
    "        # Option 2: Reshape target (choose one option, not both)\n",
    "        # target = target.view(-1, 1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:13:32.035131900Z",
     "start_time": "2023-09-16T13:04:53.193027600Z"
    }
   },
   "id": "c1d36c5b8ad23718"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Run multiple forward passes to get a distribution over outputs\n",
    "num_samples = 100\n",
    "outputs = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(num_samples):\n",
    "        output = model(X_test_tensor).squeeze().cpu().numpy()\n",
    "        outputs.append(output)\n",
    "\n",
    "# Step 2: Calculate the mean and standard deviation\n",
    "outputs = np.array(outputs)\n",
    "mean_outputs = outputs.mean(axis=0)\n",
    "std_outputs = outputs.std(axis=0)\n",
    "\n",
    "# Step 3: Visualization for a specific test sample (e.g., the first one)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(outputs[:, 0], bins=20, alpha=0.5, label='Output distribution')\n",
    "plt.axvline(mean_outputs[0], color='r', linestyle='dashed', linewidth=1, label='Mean output')\n",
    "plt.title('Output Distribution for the First Test Sample')\n",
    "plt.xlabel('Output Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Make decisions based on mean and std\n",
    "threshold_std = 0.1\n",
    "classification_results = []\n",
    "for mean, std in zip(mean_outputs, std_outputs):\n",
    "    if std < threshold_std:\n",
    "        classification_results.append((int(mean > 0.5), 'Certain'))\n",
    "    else:\n",
    "        classification_results.append((int(mean > 0.5), 'Uncertain'))\n",
    "\n",
    "# The classification_results list will contain tuples (class, certainty) for each test sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:13:46.855944200Z",
     "start_time": "2023-09-16T13:13:46.706412400Z"
    }
   },
   "id": "7e6900586c08f724"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n (0, 'Certain'),\n (1, 'Certain'),\n (0, 'Certain'),\n ...]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:13:49.474408600Z",
     "start_time": "2023-09-16T13:13:49.446715200Z"
    }
   },
   "id": "74f4e44c1d08c044"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8530631045601106\n",
      "Precision: 0.7520525451559934\n",
      "Recall: 0.5830681094844048\n",
      "F1 Score: 0.6568662603083542\n",
      "Confusion Matrix: \n",
      "[[4640  302]\n",
      " [ 655  916]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "# Convert mean outputs to binary classification labels\n",
    "predicted_labels = (mean_outputs > 0.5).astype(int)\n",
    "\n",
    "# True labels\n",
    "true_labels = y_test_tensor.cpu().numpy()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:13:50.778564300Z",
     "start_time": "2023-09-16T13:13:50.748392900Z"
    }
   },
   "id": "e6e6bb5ba3b52753"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Normal neural network in comparison:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:13:54.383443300Z",
     "start_time": "2023-09-16T13:13:54.359018400Z"
    }
   },
   "id": "9c9d454d16d46705"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.8274220789190849,\n 0.673121611154144,\n 0.5531508593252705,\n 0.6072676450034941,\n array([[4520,  422],\n        [ 702,  869]], dtype=int64))"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_data = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define a standard (non-Bayesian) Neural Network\n",
    "class StandardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StandardNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(14, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = StandardNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(500):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(X_test_tensor).squeeze().cpu().numpy()\n",
    "    predicted_labels = (test_output > 0.5).astype(int)\n",
    "    true_labels = y_test_tensor.cpu().numpy()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "accuracy, precision, recall, f1, conf_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:17:29.074742300Z",
     "start_time": "2023-09-16T13:13:55.846275500Z"
    }
   },
   "id": "ed02385f69a1be7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d443402b3054b9a"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Normal vs Bayesian performance on the same df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T13:26:16.203541100Z",
     "start_time": "2023-09-16T13:26:16.184971300Z"
    }
   },
   "id": "51ab09c533f31049"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3f66a45fb7204e38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9bf58755557b9094"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
