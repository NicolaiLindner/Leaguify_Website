{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-03T16:13:09.525626300Z",
     "start_time": "2023-10-03T16:13:09.274464300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.getenv('PREDICTION_CSV_PATH'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T16:13:23.026311200Z",
     "start_time": "2023-10-03T16:13:22.776785900Z"
    }
   },
   "id": "d62a6122b46584d2"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0.1  Unnamed: 0  Rank  Champion Name  Tier  Win rate  Pick Rate  \\\n0             0           0     1          105.0   4.0     53.87       12.0   \n1             1           1     1          105.0   4.0     53.87       12.0   \n2             2           2     1          105.0   4.0     53.87       12.0   \n3             3           3     1          105.0   4.0     53.87       12.0   \n4             4           4     1          105.0   4.0     53.87       12.0   \n\n   Ban Rate  Matches  championId  ...  role  summoner1Casts  summoner1Id  \\\n0       1.7    19216          50  ...   3.0               3            4   \n1       1.7    19216          50  ...   0.0               3            4   \n2       1.7    19216          50  ...   0.0               1            4   \n3       1.7    19216          50  ...   0.0               5            6   \n4       1.7    19216          50  ...   4.0               3           14   \n\n   summoner2Casts  summoner2Id  summonerLevel  summonerName  teamId  \\\n0               6            3            546       17645.0     100   \n1               4            6            105       20869.0     100   \n2               4            6            212       22970.0     100   \n3               3            4            681       28039.0     100   \n4               3            4            671       33854.0     200   \n\n   teamPosition  win  \n0           0.0  1.0  \n1           0.0  0.0  \n2           0.0  0.0  \n3           0.0  1.0  \n4           4.0  0.0  \n\n[5 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0</th>\n      <th>Rank</th>\n      <th>Champion Name</th>\n      <th>Tier</th>\n      <th>Win rate</th>\n      <th>Pick Rate</th>\n      <th>Ban Rate</th>\n      <th>Matches</th>\n      <th>championId</th>\n      <th>...</th>\n      <th>role</th>\n      <th>summoner1Casts</th>\n      <th>summoner1Id</th>\n      <th>summoner2Casts</th>\n      <th>summoner2Id</th>\n      <th>summonerLevel</th>\n      <th>summonerName</th>\n      <th>teamId</th>\n      <th>teamPosition</th>\n      <th>win</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>105.0</td>\n      <td>4.0</td>\n      <td>53.87</td>\n      <td>12.0</td>\n      <td>1.7</td>\n      <td>19216</td>\n      <td>50</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>3</td>\n      <td>4</td>\n      <td>6</td>\n      <td>3</td>\n      <td>546</td>\n      <td>17645.0</td>\n      <td>100</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>105.0</td>\n      <td>4.0</td>\n      <td>53.87</td>\n      <td>12.0</td>\n      <td>1.7</td>\n      <td>19216</td>\n      <td>50</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>105</td>\n      <td>20869.0</td>\n      <td>100</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>105.0</td>\n      <td>4.0</td>\n      <td>53.87</td>\n      <td>12.0</td>\n      <td>1.7</td>\n      <td>19216</td>\n      <td>50</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>212</td>\n      <td>22970.0</td>\n      <td>100</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>105.0</td>\n      <td>4.0</td>\n      <td>53.87</td>\n      <td>12.0</td>\n      <td>1.7</td>\n      <td>19216</td>\n      <td>50</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>6</td>\n      <td>3</td>\n      <td>4</td>\n      <td>681</td>\n      <td>28039.0</td>\n      <td>100</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>105.0</td>\n      <td>4.0</td>\n      <td>53.87</td>\n      <td>12.0</td>\n      <td>1.7</td>\n      <td>19216</td>\n      <td>50</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>3</td>\n      <td>14</td>\n      <td>3</td>\n      <td>4</td>\n      <td>671</td>\n      <td>33854.0</td>\n      <td>200</td>\n      <td>4.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 27 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T16:13:25.658382300Z",
     "start_time": "2023-10-03T16:13:25.645004600Z"
    }
   },
   "id": "2ccab8002d9dd49e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0', 'Unnamed: 0.1'], inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T16:15:10.209132100Z",
     "start_time": "2023-10-03T16:15:10.193193600Z"
    }
   },
   "id": "7f415a6758302d25"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(Rank               0\n Champion Name      0\n Tier               0\n Win rate           0\n Pick Rate          0\n Ban Rate           0\n Matches            0\n championId         0\n championName       0\n lane               0\n playerAugment1     0\n playerAugment2     0\n playerAugment3     0\n playerAugment4     0\n playerSubteamId    0\n role               0\n summoner1Casts     0\n summoner1Id        0\n summoner2Casts     0\n summoner2Id        0\n summonerLevel      0\n summonerName       0\n teamId             0\n teamPosition       0\n win                0\n dtype: int64,\n (201901, 24),\n (50476, 24))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = df.drop('win', axis=1)\n",
    "y = df['win']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "missing_values, X_train.shape, X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T16:20:37.641713500Z",
     "start_time": "2023-10-03T16:20:37.384845100Z"
    }
   },
   "id": "5c592383b63118e8"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\xgboost\\data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.5775616134400507"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T16:21:56.810719400Z",
     "start_time": "2023-10-03T16:21:56.243865Z"
    }
   },
   "id": "7435a3cb710d8a68"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.9307\n",
      "Epoch [20/1000], Loss: 0.8038\n",
      "Epoch [30/1000], Loss: 0.7327\n",
      "Epoch [40/1000], Loss: 0.7418\n",
      "Epoch [50/1000], Loss: 0.7174\n",
      "Epoch [60/1000], Loss: 0.7134\n",
      "Epoch [70/1000], Loss: 0.7143\n",
      "Epoch [80/1000], Loss: 0.7102\n",
      "Epoch [90/1000], Loss: 0.7102\n",
      "Epoch [100/1000], Loss: 0.7092\n",
      "Epoch [110/1000], Loss: 0.7016\n",
      "Epoch [120/1000], Loss: 0.7002\n",
      "Epoch [130/1000], Loss: 0.7088\n",
      "Epoch [140/1000], Loss: 0.7051\n",
      "Epoch [150/1000], Loss: 0.6997\n",
      "Epoch [160/1000], Loss: 0.7011\n",
      "Epoch [170/1000], Loss: 0.6997\n",
      "Epoch [180/1000], Loss: 0.6992\n",
      "Epoch [190/1000], Loss: 0.7005\n",
      "Epoch [200/1000], Loss: 0.6986\n",
      "Epoch [210/1000], Loss: 0.6997\n",
      "Epoch [220/1000], Loss: 0.6969\n",
      "Epoch [230/1000], Loss: 0.6956\n",
      "Epoch [240/1000], Loss: 0.7070\n",
      "Epoch [250/1000], Loss: 0.6965\n",
      "Epoch [260/1000], Loss: 0.6959\n",
      "Epoch [270/1000], Loss: 0.6961\n",
      "Epoch [280/1000], Loss: 0.6982\n",
      "Epoch [290/1000], Loss: 0.6956\n",
      "Epoch [300/1000], Loss: 0.6969\n",
      "Epoch [310/1000], Loss: 0.6971\n",
      "Epoch [320/1000], Loss: 0.6958\n",
      "Epoch [330/1000], Loss: 0.6957\n",
      "Epoch [340/1000], Loss: 0.6944\n",
      "Epoch [350/1000], Loss: 0.6944\n",
      "Epoch [360/1000], Loss: 0.6952\n",
      "Epoch [370/1000], Loss: 0.6969\n",
      "Epoch [380/1000], Loss: 0.6995\n",
      "Epoch [390/1000], Loss: 0.6946\n",
      "Epoch [400/1000], Loss: 0.6944\n",
      "Epoch [410/1000], Loss: 0.6963\n",
      "Epoch [420/1000], Loss: 0.6943\n",
      "Epoch [430/1000], Loss: 0.6939\n",
      "Epoch [440/1000], Loss: 0.7058\n",
      "Epoch [450/1000], Loss: 0.6938\n",
      "Epoch [460/1000], Loss: 0.6951\n",
      "Epoch [470/1000], Loss: 0.6958\n",
      "Epoch [480/1000], Loss: 0.6936\n",
      "Epoch [490/1000], Loss: 0.6935\n",
      "Epoch [500/1000], Loss: 0.6938\n",
      "Epoch [510/1000], Loss: 0.6933\n",
      "Epoch [520/1000], Loss: 0.6931\n",
      "Epoch [530/1000], Loss: 0.6977\n",
      "Epoch [540/1000], Loss: 0.6940\n",
      "Epoch [550/1000], Loss: 0.6975\n",
      "Epoch [560/1000], Loss: 0.6939\n",
      "Epoch [570/1000], Loss: 0.6954\n",
      "Epoch [580/1000], Loss: 0.6954\n",
      "Epoch [590/1000], Loss: 0.6930\n",
      "Epoch [600/1000], Loss: 0.6931\n",
      "Epoch [610/1000], Loss: 0.6960\n",
      "Epoch [620/1000], Loss: 0.6937\n",
      "Epoch [630/1000], Loss: 0.6931\n",
      "Epoch [640/1000], Loss: 0.6932\n",
      "Epoch [650/1000], Loss: 0.6927\n",
      "Epoch [660/1000], Loss: 0.6924\n",
      "Epoch [670/1000], Loss: 0.6966\n",
      "Epoch [680/1000], Loss: 0.6936\n",
      "Epoch [690/1000], Loss: 0.6920\n",
      "Epoch [700/1000], Loss: 0.6922\n",
      "Epoch [710/1000], Loss: 0.6933\n",
      "Epoch [720/1000], Loss: 0.6926\n",
      "Epoch [730/1000], Loss: 0.6966\n",
      "Epoch [740/1000], Loss: 0.6927\n",
      "Epoch [750/1000], Loss: 0.6930\n",
      "Epoch [760/1000], Loss: 0.6921\n",
      "Epoch [770/1000], Loss: 0.6935\n",
      "Epoch [780/1000], Loss: 0.6921\n",
      "Epoch [790/1000], Loss: 0.6925\n",
      "Epoch [800/1000], Loss: 0.6923\n",
      "Epoch [810/1000], Loss: 0.6930\n",
      "Epoch [820/1000], Loss: 0.6917\n",
      "Epoch [830/1000], Loss: 0.6931\n",
      "Epoch [840/1000], Loss: 0.6930\n",
      "Epoch [850/1000], Loss: 0.6917\n",
      "Epoch [860/1000], Loss: 0.6923\n",
      "Epoch [870/1000], Loss: 0.6913\n",
      "Epoch [880/1000], Loss: 0.6927\n",
      "Epoch [890/1000], Loss: 0.6913\n",
      "Epoch [900/1000], Loss: 0.6932\n",
      "Epoch [910/1000], Loss: 0.6928\n",
      "Epoch [920/1000], Loss: 0.6911\n",
      "Epoch [930/1000], Loss: 0.6916\n",
      "Epoch [940/1000], Loss: 0.6913\n",
      "Epoch [950/1000], Loss: 0.6917\n",
      "Epoch [960/1000], Loss: 0.6908\n",
      "Epoch [970/1000], Loss: 0.6910\n",
      "Epoch [980/1000], Loss: 0.6912\n",
      "Epoch [990/1000], Loss: 0.6906\n",
      "Epoch [1000/1000], Loss: 0.6911\n",
      "Accuracy on the test set: 0.5241\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn, get_kl_loss\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a Bayesian neural network architecture with more complexity\n",
    "class VeryLargeBayesianNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size1, hidden_size2):\n",
    "        super(VeryLargeBayesianNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size2)\n",
    "        self.fc4 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Convert a deterministic neural network to Bayesian\n",
    "def convert_to_bnn(model):\n",
    "    const_bnn_prior_parameters = {\n",
    "        \"prior_mu\": 0.0,\n",
    "        \"prior_sigma\": 1.0,\n",
    "        \"posterior_mu_init\": 0.0,\n",
    "        \"posterior_rho_init\": -3.0,\n",
    "        \"type\": \"Reparameterization\",  # Flipout or Reparameterization\n",
    "        \"moped_enable\": False,  # True to initialize mu/sigma from the pretrained dnn weights\n",
    "        \"moped_delta\": 0.5,\n",
    "    }\n",
    "    \n",
    "    dnn_to_bnn(model, const_bnn_prior_parameters)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 1  # Assuming you are predicting a binary outcome (win or not)\n",
    "hidden_size1 = 512  # Increased size of the first hidden layer\n",
    "hidden_size2 = 256  # Added a second hidden layer with more neurons\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000  # Increased number of training epochs\n",
    "\n",
    "# Create the larger Bayesian neural network\n",
    "very_large_bnn_model = VeryLargeBayesianNN(input_size, output_size, hidden_size1, hidden_size2)\n",
    "\n",
    "# Convert the model to Bayesian\n",
    "convert_to_bnn(very_large_bnn_model)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(very_large_bnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ... (previous code for preprocessing)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)  # Reshape to (number_of_samples, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)  # Reshape to (number_of_samples, 1)\n",
    "\n",
    "# ... (rest of the code)\n",
    "\n",
    "# Training loop with increased number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = very_large_bnn_model(X_train_tensor)\n",
    "    \n",
    "    # Compute the KL divergence loss\n",
    "    kl_loss = get_kl_loss(very_large_bnn_model)\n",
    "    \n",
    "    # Compute the binary cross-entropy loss\n",
    "    loss = criterion(outputs, y_train_tensor) + kl_loss / len(X_train_tensor)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss for monitoring\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "very_large_bnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Make predictions on the test set\n",
    "    test_outputs = very_large_bnn_model(X_test_tensor)\n",
    "    predicted_labels = (torch.sigmoid(test_outputs) >= 0.5).float()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# You can now use the model to make predictions on new data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T16:48:12.221919Z",
     "start_time": "2023-10-03T16:31:51.090282500Z"
    }
   },
   "id": "591ab07d2c9a3915"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "271c30e5c8908dc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
