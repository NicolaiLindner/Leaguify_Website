{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Building a more efficient scrapy scraper"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T10:22:40.459272900Z",
     "start_time": "2023-09-25T10:22:40.456534200Z"
    }
   },
   "id": "dac6447885c1b07f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 12:22:40 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-09-25 12:22:40 [py.warnings] WARNING: C:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-09-25 12:22:40 [scrapy.extensions.telnet] INFO: Telnet Password: 85cec191b1b9035b\n",
      "2023-09-25 12:22:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-25 12:22:41 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 'INFO'}\n",
      "2023-09-25 12:22:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-25 12:22:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-25 12:22:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-25 12:22:41 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-25 12:22:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-25 12:22:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-25 12:22:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-25 12:22:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 245,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 65050,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.240817,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 25, 10, 22, 41, 360608, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 338487,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 9, 25, 10, 22, 41, 119791, tzinfo=datetime.timezone.utc)}\n",
      "2023-09-25 12:22:41 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 62\u001B[0m\n\u001B[0;32m     60\u001B[0m summoner_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleaguify\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     61\u001B[0m region \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meuw1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 62\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mrun_spider\u001B[49m\u001B[43m(\u001B[49m\u001B[43msummoner_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28mprint\u001B[39m(result)  \u001B[38;5;66;03m# This is just for testing; remove it when you integrate this code elsewhere\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[2], line 56\u001B[0m, in \u001B[0;36mrun_spider\u001B[1;34m(summoner_name, region)\u001B[0m\n\u001B[0;32m     52\u001B[0m reactor\u001B[38;5;241m.\u001B[39mrun()\n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# At this point, the spider instance is stored in the runner's crawlers attribute\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# Retrieve the spider data from there\u001B[39;00m\n\u001B[1;32m---> 56\u001B[0m spider \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mrunner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcrawlers\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mspider  \u001B[38;5;66;03m# Convert set to list and then get the first element\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m spider\u001B[38;5;241m.\u001B[39mdata\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "from scrapy.spiders import Spider\n",
    "from twisted.internet import defer\n",
    "\n",
    "class MySpider(Spider):\n",
    "    name = 'my_spider'\n",
    "    custom_settings = {'LOG_LEVEL': 'INFO'}\n",
    "\n",
    "    def __init__(self, summoner_name, region, *args, **kwargs):\n",
    "        super(MySpider, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [f\"https://u.gg/lol/profile/{region}/{summoner_name}/champion-stats\"]\n",
    "        self.data = {}\n",
    "        self.columns = ['Rank', 'Champion', 'Win Rate', 'Wins/Loses', 'Unnamed', 'Kills', 'Deaths', 'Assists', 'LP',\n",
    "                        'Max Kills', 'Max Deaths', 'CS', 'Damage', 'Gold']\n",
    "\n",
    "    def parse(self, response):\n",
    "        try:\n",
    "            row = {}\n",
    "            selectors = [\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(1) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(2) > div:nth-child(1) > span:nth-child(2)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(3) > div:nth-child(1) > strong:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(3) > div:nth-child(1) > span:nth-child(3)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(4) > div:nth-child(1) > div:nth-child(1) > strong:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(4) > div:nth-child(1) > span:nth-child(2) > strong:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(4) > div:nth-child(1) > span:nth-child(2) > strong:nth-child(3)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(4) > div:nth-child(1) > span:nth-child(2) > strong:nth-child(5)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(5) > span:nth-child(1) > span:nth-child(2)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(6) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(7) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(8) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(9) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(10) > span:nth-child(1)::text\",\n",
    "            ]\n",
    "            for col, selector in zip(self.columns, selectors):\n",
    "                item = response.css(selector).get()\n",
    "                row[col] = item.strip() if item else 'N/A'\n",
    "            self.data.update(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log(f\"Error: {e}\")\n",
    "\n",
    "    def closed(self, reason):\n",
    "        reactor.stop()\n",
    "\n",
    "def run_spider(summoner_name, region):\n",
    "    configure_logging({'LOG_LEVEL': 'INFO'})\n",
    "    runner = CrawlerRunner()\n",
    "    d = runner.crawl(MySpider, summoner_name=summoner_name, region=region)\n",
    "    d.addBoth(lambda _: reactor.stop())\n",
    "    reactor.run()\n",
    "    print(\"Runner crawlers:\", runner.crawlers)  # Debugging line\n",
    "\n",
    "\n",
    "    # At this point, the spider instance is stored in the runner's crawlers attribute\n",
    "    # Retrieve the spider data from there\n",
    "    spider = list(runner.crawlers)[0].spider  # Convert set to list and then get the first element\n",
    "    return spider.data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    summoner_name = \"leaguify\"\n",
    "    region = \"euw1\"\n",
    "    result = run_spider(summoner_name, region)\n",
    "    print(result)  # This is just for testing; remove it when you integrate this code elsewhere"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T10:22:41.463299500Z",
     "start_time": "2023-09-25T10:22:40.461273300Z"
    }
   },
   "id": "80f32cce483ed2df"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 12:28:09 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-09-25 12:28:09 [py.warnings] WARNING: C:\\Programmieren\\Leaguify_Website\\venv\\Lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-09-25 12:28:09 [scrapy.extensions.telnet] INFO: Telnet Password: 8de3ade976199422\n",
      "2023-09-25 12:28:09 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-25 12:28:09 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 'INFO'}\n",
      "2023-09-25 12:28:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-25 12:28:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-25 12:28:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-25 12:28:09 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-25 12:28:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-25 12:28:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-25 12:28:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-25 12:28:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 245,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 65050,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.267448,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 25, 10, 28, 10, 192135, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 338487,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 9, 25, 10, 28, 9, 924687, tzinfo=datetime.timezone.utc)}\n",
      "2023-09-25 12:28:10 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runner crawlers: set()\n",
      "No crawlers were run.\n"
     ]
    }
   ],
   "source": [
    "from twisted.internet import reactor, defer\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "from scrapy.spiders import Spider\n",
    "\n",
    "class MySpider(Spider):\n",
    "    name = 'my_spider'\n",
    "    custom_settings = {'LOG_LEVEL': 'INFO'}\n",
    "\n",
    "    def __init__(self, summoner_name, region, *args, **kwargs):\n",
    "        super(MySpider, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [f\"https://u.gg/lol/profile/{region}/{summoner_name}/champion-stats\"]\n",
    "        self.data = {}\n",
    "        self.columns = ['Rank', 'Champion', 'Win Rate', 'Wins/Loses', 'Unnamed', 'Kills', 'Deaths', 'Assists', 'LP',\n",
    "                        'Max Kills', 'Max Deaths', 'CS', 'Damage', 'Gold']\n",
    "\n",
    "    def parse(self, response):\n",
    "        try:\n",
    "            row = {}\n",
    "            selectors = [\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(1) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(2) > div:nth-child(1) > span:nth-child(2)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(3) > div:nth-child(1) > strong:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(3) > div:nth-child(1) > span:nth-child(3)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(4) > div:nth-child(1) > div:nth-child(1) > strong:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(4) > div:nth-child(1) > span:nth-child(2) > strong:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(4) > div:nth-child(1) > span:nth-child(2) > strong:nth-child(3)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(4) > div:nth-child(1) > span:nth-child(2) > strong:nth-child(5)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(5) > span:nth-child(1) > span:nth-child(2)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(6) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(7) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(8) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(9) > span:nth-child(1)::text\",\n",
    "                \"div.rt-tr-group:nth-child(1) > div:nth-child(1) > div:nth-child(10) > span:nth-child(1)::text\",\n",
    "            ]\n",
    "            for col, selector in zip(self.columns, selectors):\n",
    "                item = response.css(selector).get()\n",
    "                row[col] = item.strip() if item else 'N/A'\n",
    "            self.data.update(row)\n",
    "        except Exception as e:\n",
    "            self.log(f\"Error: {e}\")\n",
    "\n",
    "    def closed(self, reason):\n",
    "        reactor.stop()\n",
    "\n",
    "@defer.inlineCallbacks\n",
    "def run_spider(summoner_name, region):\n",
    "    runner = CrawlerRunner()\n",
    "    yield runner.crawl(MySpider, summoner_name=summoner_name, region=region)\n",
    "    \n",
    "    # Debugging line to check the contents of runner.crawlers\n",
    "    print(\"Runner crawlers:\", runner.crawlers)\n",
    "    \n",
    "    if runner.crawlers:\n",
    "        spider = list(runner.crawlers)[0].spider  # Convert set to list and then get the first element\n",
    "        print(spider.data)  # This is just for testing; remove it when you integrate this code elsewhere\n",
    "    else:\n",
    "        print(\"No crawlers were run.\")\n",
    "    reactor.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    configure_logging({'LOG_LEVEL': 'INFO'})\n",
    "    summoner_name = \"leaguify\"\n",
    "    region = \"euw1\"\n",
    "    \n",
    "    run_spider(summoner_name, region)\n",
    "    reactor.run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T10:28:10.204119200Z",
     "start_time": "2023-09-25T10:28:09.299454600Z"
    }
   },
   "id": "61316c77b04cc3ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1311f7b1d3bbac8e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
